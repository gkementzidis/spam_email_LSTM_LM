{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gkementzidis/spam_email_LSTM_LM/blob/main/spam_detection_LLM_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZUp-mW0TDjG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer, TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "wy70neq6TxgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "eK-PtpvaVeI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyIOi4m9TDjH"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeBxVzJnTDjI"
      },
      "outputs": [],
      "source": [
        "# loading data\n",
        "data = pd.read_csv('/content/drive/MyDrive/emails.csv')\n",
        "print(data.shape)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnRp4iaxTDjJ"
      },
      "source": [
        "### Class imbalance\n",
        "\n",
        "In updated versions of this file, we will address class imbalance and how to tackle possible adverse effects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGjpxcGOTDjJ"
      },
      "outputs": [],
      "source": [
        "data['spam'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmznsBPMTDjJ"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "In contrast with the LSTM version, I chose to keep many stopwords in the text, since the attention mechanism will handle their relative importance. Sometimes stopwords such as \"and\" or \"through\" do add significant meaning to the text, and help us avoid misunderstandings. Also, I don't need to turn upper case letters to lower case, since the GPT-2 tokenizer will handle them accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWBM67wwTDjJ"
      },
      "outputs": [],
      "source": [
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiBd5TExTDjJ"
      },
      "source": [
        "## Tokenizer\n",
        "\n",
        "We are using the GPT-2 tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbSxP_c9TDjJ"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTwKJOMJTDjJ"
      },
      "source": [
        "### Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgDD1zsJTDjJ"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize text\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTcVdD8uTDjJ"
      },
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "dataset = CustomDataset(\n",
        "    texts=data[\"text\"].tolist(),\n",
        "    labels=data[\"spam\"].tolist(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=tokenizer.model_max_length\n",
        ")\n",
        "\n",
        "# train-test split\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# DataLoader for batching\n",
        "# train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuTtCcR3TDjJ"
      },
      "source": [
        "## GPT-2 Model\n",
        "\n",
        "GPT-2 stands for \"Generative Pre-trained Transformer 2\". Developed by OpenAI, it is a transformer-based model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szKoPVOQTDjJ"
      },
      "outputs": [],
      "source": [
        "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "model.config.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2pPFPgFTDjJ"
      },
      "source": [
        "### Evaluation metrics\n",
        "\n",
        "Accuracy, precision, recall, and F1-score are typical metrics used in classification tasks. Especially when it comes to imbalanced datasets, accuracy alone is not a good indicator of whether the model is trained well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9L0awxCTDjJ"
      },
      "outputs": [],
      "source": [
        "# Load metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "recall = evaluate.load(\"recall\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "# Compute multiple metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Calculate each metric\n",
        "    acc = accuracy.compute(predictions=predictions, references=labels)\n",
        "    prec = precision.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    rec = recall.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
        "\n",
        "    # Return a dictionary of all metrics\n",
        "    return {\n",
        "        \"accuracy\": acc[\"accuracy\"],\n",
        "        \"precision\": prec[\"precision\"],\n",
        "        \"recall\": rec[\"recall\"],\n",
        "        \"f1\": f1_score[\"f1\"]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9W9hX6xTDjK"
      },
      "source": [
        "### Training setup\n",
        "\n",
        "I am using a very small batch size (1) due to compute limitations. By default the model uses the Adam optimizer with a linear schedule for the specified (by us) learning rate. Also, the loss function that is used is binary cross entropy, a golden standard for classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJ9zssJnTDjK"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\",\n",
        "    eval_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    num_train_epochs=2,\n",
        "    save_strategy=\"epoch\",  # Save the model every epoch\n",
        "    learning_rate=5e-5,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,  # Log every 50 steps\n",
        ")\n",
        "\n",
        "# Define trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,  # Replace with your train dataset\n",
        "    eval_dataset=test_dataset,  # Replace with your eval dataset\n",
        "    tokenizer=tokenizer,  # Add tokenizer for data collators\n",
        "    compute_metrics=compute_metrics  # Optional: Function to calculate accuracy, etc.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU found.\")"
      ],
      "metadata": {
        "id": "K2K6wzeVZjcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"COLAB_TPU_ADDR\" in os.environ:\n",
        "    print(\"TPU is available!\")\n",
        "else:\n",
        "    print(\"No TPU found.\")"
      ],
      "metadata": {
        "id": "FKhN6hysasJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "gp3vt3zvbgbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtoVb8sKTDjK"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJqnb-W-TDjK"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "Ib84w_-6WD_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will address the results in more detail in the future. However, so far they look much better than the LSTM model I designed a few months ago."
      ],
      "metadata": {
        "id": "6CusDzYurM9u"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rgan_pytorch",
      "language": "python",
      "name": "rgan_pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}