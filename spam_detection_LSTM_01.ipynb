{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - Spam email detection\n",
    "## G. Kementzidis\n",
    "Let's create an LSTM network using PyTorch. The goal is to classify emails as either spam or not spam based on their text.\n",
    "\n",
    "A lot of people have trained similar models to achieve this task: from simpler concepts like Naive Bayes to more complicated architectures like tranformers.\n",
    "\n",
    "I found a labeled dataset on Kaggle with emails, and whether they are considered spam or not. You can find it here: https://www.kaggle.com/datasets/jackksoncsie/spam-email-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15f11a4d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LSTM, Linear\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "data = pd.read_csv('emails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest email has 8477 words\n"
     ]
    }
   ],
   "source": [
    "# create a histogram with the length of each email\n",
    "lengths = [len(data['text'][i].split()) for i in range(len(data['text']))]\n",
    "\n",
    "plt.hist(lengths, bins=150)\n",
    "plt.title('Word counts in emails')\n",
    "plt.show()\n",
    "\n",
    "max_length = max(lengths)\n",
    "print('The longest email has', max_length, 'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the texts\n",
    "- Remove punctuation\n",
    "- Turn upper-case characters to lower-case\n",
    "\n",
    "We will be using the first 100 words of the emails with at least 100 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "      <th>text_mod</th>\n",
       "      <th>text_cut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject naturally irresistible your corporate ...</td>\n",
       "      <td>subject naturally irresistible your corporate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: here ' s a hot play in motion  homela...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject here  s a hot play in motion  homeland...</td>\n",
       "      <td>subject here s a hot play in motion homeland s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: undeliverable : home based business f...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject undeliverable  home based business for...</td>\n",
       "      <td>subject undeliverable home based business for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: las vegas high rise boom  las vegas i...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject las vegas high rise boom  las vegas is...</td>\n",
       "      <td>subject las vegas high rise boom las vegas is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: brighten those teeth  get your  teeth...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject brighten those teeth  get your  teeth ...</td>\n",
       "      <td>subject brighten those teeth get your teeth br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4321</th>\n",
       "      <td>Subject: re : research and development charges...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject re  research and development charges t...</td>\n",
       "      <td>subject re research and development charges to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4322</th>\n",
       "      <td>Subject: re : receipts from visit  jim ,  than...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject re  receipts from visit  jim   thanks ...</td>\n",
       "      <td>subject re receipts from visit jim thanks agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4323</th>\n",
       "      <td>Subject: re : enron case study update  wow ! a...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject re  enron case study update  wow  all ...</td>\n",
       "      <td>subject re enron case study update wow all on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4324</th>\n",
       "      <td>Subject: re : interest  david ,  please , call...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject re  interest  david   please  call shi...</td>\n",
       "      <td>subject re interest david please call shirley ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>Subject: news : aurora 5 . 2 update  aurora ve...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject news  aurora    update  aurora version...</td>\n",
       "      <td>subject news aurora update aurora version the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4326 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  spam  \\\n",
       "0     Subject: naturally irresistible your corporate...     1   \n",
       "1     Subject: here ' s a hot play in motion  homela...     1   \n",
       "2     Subject: undeliverable : home based business f...     1   \n",
       "3     Subject: las vegas high rise boom  las vegas i...     1   \n",
       "4     Subject: brighten those teeth  get your  teeth...     1   \n",
       "...                                                 ...   ...   \n",
       "4321  Subject: re : research and development charges...     0   \n",
       "4322  Subject: re : receipts from visit  jim ,  than...     0   \n",
       "4323  Subject: re : enron case study update  wow ! a...     0   \n",
       "4324  Subject: re : interest  david ,  please , call...     0   \n",
       "4325  Subject: news : aurora 5 . 2 update  aurora ve...     0   \n",
       "\n",
       "                                               text_mod  \\\n",
       "0     subject naturally irresistible your corporate ...   \n",
       "1     subject here  s a hot play in motion  homeland...   \n",
       "2     subject undeliverable  home based business for...   \n",
       "3     subject las vegas high rise boom  las vegas is...   \n",
       "4     subject brighten those teeth  get your  teeth ...   \n",
       "...                                                 ...   \n",
       "4321  subject re  research and development charges t...   \n",
       "4322  subject re  receipts from visit  jim   thanks ...   \n",
       "4323  subject re  enron case study update  wow  all ...   \n",
       "4324  subject re  interest  david   please  call shi...   \n",
       "4325  subject news  aurora    update  aurora version...   \n",
       "\n",
       "                                               text_cut  \n",
       "0     subject naturally irresistible your corporate ...  \n",
       "1     subject here s a hot play in motion homeland s...  \n",
       "2     subject undeliverable home based business for ...  \n",
       "3     subject las vegas high rise boom las vegas is ...  \n",
       "4     subject brighten those teeth get your teeth br...  \n",
       "...                                                 ...  \n",
       "4321  subject re research and development charges to...  \n",
       "4322  subject re receipts from visit jim thanks agai...  \n",
       "4323  subject re enron case study update wow all on ...  \n",
       "4324  subject re interest david please call shirley ...  \n",
       "4325  subject news aurora update aurora version the ...  \n",
       "\n",
       "[4326 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = 100\n",
    "\n",
    "# remove punctuation and make all letters lowercase\n",
    "# we use regular expressions\n",
    "data['text_mod'] = data['text'].str.replace(r'[^a-zA-Z\\s]', '', regex=True).str.lower()\n",
    "more_than_100 = np.array([1 if len(data['text'][i].split()) > num_words else 0 for i in range(len(data['text']))])\n",
    "data_hundred = data[more_than_100 == 1].reset_index(drop=True)\n",
    "\n",
    "# get the first 100 words of each email\n",
    "data_hundred['text_cut'] = data_hundred['text_mod'].str.split().apply(lambda x: x[:num_words]).str.join(' ')\n",
    "data_hundred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make sure to remove duplicates and check whether our dataset is balanced. As you'll see later at the bottom of the notebook, during our first attempts, the dataset was unbalanced and the results were mixed. To balance it, we only kept 2/3 of the non-spam emails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates and reset index\n",
    "data_hundred = data_hundred.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam\n",
       "0    3438\n",
       "1     861\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hundred['spam'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spam\n",
       "0    2292\n",
       "1     861\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from data_hundred, keep only 2/3 of the non-spam emails\n",
    "data_hundred = data_hundred.drop(data_hundred[data_hundred['spam'] == 0].sample(frac=1/3).index).reset_index(drop=True)\n",
    "data_hundred['spam'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a variety of methods to deal with unbalanced datasets. In the case of images or numerical variables, you can conduct some sort of data augmentation, such as using reflections or rotations of images, SMOTE (synthetic minortiy over-sampling technique) for numerical data, etc. These cannot be easily applied to our model, because our input to the model is a text/string, whose words are carefully placed one after the other to convey a very specific meaning. Repeating words, bootstraping, or data-points interpolation isn't very meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating word embeddings\n",
    "We gather all unique words from the emails and we encode each one of them with a unique $x \\in \\R^{100}$. Whether the vector length is 100 or some other number (50, 150, 200, etc.) is a matter of choice. The word length is a hyperparameters and we can try different values. \n",
    "\n",
    "We are using a pre-trained embedding called GloVe which respects the semantic and syntactic relationships between words. A previous attempt involved a random word embedding and there was little information learnt by the LSTM model.\n",
    "\n",
    "[GloVe: Global Vectors for Word Representation](https://aclanthology.org/D14-1162) (Pennington et al., EMNLP 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "glove_path = 'glove.6B.100d.txt'\n",
    "glove = {}\n",
    "with open(glove_path, 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], 'float32')\n",
    "        glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_length = 100\n",
    "\n",
    "# create a dictionary of all unique words\n",
    "word_dict = {}\n",
    "num = 2\n",
    "for i in range(len(data_hundred)):\n",
    "    for word in data_hundred['text_cut'][i].split():\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = num\n",
    "            num += 1\n",
    "\n",
    "word_dict['<PAD>'] = 0 # padding\n",
    "word_dict['<UNK>'] = 1 # unknown word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word embeddings for each word\n",
    "# Create an embedding matrix\n",
    "embedding_matrix = np.zeros((len(word_dict), word_length))\n",
    "for word, i in word_dict.items():\n",
    "    embedding_vector = glove.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(word_length, ))\n",
    "\n",
    "# Convert the embedding matrix to a tensor\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "# Create an embedding layer using the GloVe embeddings\n",
    "word_embeddings = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split and create the Dataloaders\n",
    "\n",
    "An important aspect of ML is to be able to train a data-driven model which can be applied to unseen data. \n",
    "\n",
    "A train-test split (80-20) lets us see whether we are dealing with either underfitting or overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train - test split (80-20)\n",
    "train = data_hundred.sample(frac=0.8, random_state=1)\n",
    "test = data_hundred.drop(train.index)\n",
    "\n",
    "train_reset = train.reset_index(drop=False)\n",
    "test_reset = test.reset_index(drop=False)\n",
    "\n",
    "# create a tensor with the labels\n",
    "train_labels = torch.tensor(train_reset['spam'].to_numpy()).reshape(-1, 1)\n",
    "test_labels = torch.tensor(test_reset['spam'].to_numpy()).reshape(-1, 1)\n",
    "\n",
    "# create a tensor with the first num_words word embeddings of each email\n",
    "train_tensor = torch.zeros(len(train), num_words, word_length)\n",
    "test_tensor = torch.zeros(len(test), num_words, word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f6/znb06gy13c5c28v99r1ds2fh0000gn/T/ipykernel_50068/2700289647.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_tensor[i, j, :] = torch.tensor(word_embeddings(torch.tensor([word_dict.get(word, word_dict['<UNK>'])]))[0])\n",
      "/var/folders/f6/znb06gy13c5c28v99r1ds2fh0000gn/T/ipykernel_50068/2700289647.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_tensor[i, j, :] = torch.tensor(word_embeddings(torch.tensor([word_dict.get(word, word_dict['<UNK>'])]))[0])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train)):\n",
    "    for j, word in enumerate(train_reset['text_cut'][i].split()):\n",
    "        train_tensor[i, j, :] = torch.tensor(word_embeddings(torch.tensor([word_dict.get(word, word_dict['<UNK>'])]))[0])\n",
    "\n",
    "for i in range(len(test)):\n",
    "    for j, word in enumerate(test_reset['text_cut'][i].split()):\n",
    "        test_tensor[i, j, :] = torch.tensor(word_embeddings(torch.tensor([word_dict.get(word, word_dict['<UNK>'])]))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the model's hyperparameters is the batch size. We can experiment with multiple batch sizes e.g., 16, 32, 64, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders for the training and test sets\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(train_tensor, train_labels), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(test_tensor, test_labels), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the LSTM network and the final model architecture\n",
    "We add some fully connected layers at the end to get a scalar value in the range $[0,1]$, indicating the probability our email is spam (1) or non-spam (0). The sigmoid activation function at the end ensures that the final output entry would be a number in that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size=word_length, hidden_size=10, num_layers=1, batch_first=True):\n",
    "        super(Network, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # our LSTM model\n",
    "        self.LSTM = LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=batch_first)\n",
    "        # final fully connected layers\n",
    "        self.fc1 = Linear(10, 5)\n",
    "        self.fc2 = Linear(5, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        output, hidden = self.LSTM(x, (h0.detach(), c0.detach()))\n",
    "        x = F.relu(self.fc1(output[:,-1,:].view(x.size(0), -1)))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x, hidden\n",
    "    \n",
    "model = Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working on a classification task, we will be using Binary Cross Entropy as our loss function:\n",
    "\n",
    "$$-{(y\\log(p) + (1 - y)\\log(1 - p))}$$\n",
    "\n",
    "where $y$ is the class label (0 or 1) and $p$ is the output of the model - the probability that the input text belongs to class 1.\n",
    "\n",
    "For our optimization, we will be using Adam, a variation of mini-batch Stochastic Gradient Descent (SGD), which combines elements of Momentun SGD and RMSProp. The $\\beta$ parameters of our Adam algorithm are the PyTorch default values $(0.9, 0.999)$, which are commonly used for such tasks.\n",
    "\n",
    "A cruicial hyperparameter is the learning rate. My first attempts used 0.001 and the results were quite disappointing. I switched to 0.0001 and the results are much better. As expected, training with a smaller learning rate is a bit slower since the model parameters are updated by a smaller amount at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use binary cross entropy loss and the Adam optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Another important hyperparameter is the number of epochs. Experimenting with various values, we chose to train the model for 60 epochs. Thankfully, the training doesn't take too long. For longer training sessions, we would have to employ methods like early stopping, where we assign a big maximum number of epochs, and we let the model stop training when we believe that the loss function isn't reducing too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: 0.576\n",
      "[2] loss: 0.565\n",
      "[3] loss: 0.549\n",
      "[4] loss: 0.529\n",
      "[5] loss: 0.508\n",
      "[6] loss: 0.491\n",
      "[7] loss: 0.479\n",
      "[8] loss: 0.469\n",
      "[9] loss: 0.460\n",
      "[10] loss: 0.453\n",
      "[11] loss: 0.448\n",
      "[12] loss: 0.445\n",
      "[13] loss: 0.441\n",
      "[14] loss: 0.438\n",
      "[15] loss: 0.434\n",
      "[16] loss: 0.430\n",
      "[17] loss: 0.426\n",
      "[18] loss: 0.421\n",
      "[19] loss: 0.414\n",
      "[20] loss: 0.406\n",
      "[21] loss: 0.396\n",
      "[22] loss: 0.383\n",
      "[23] loss: 0.364\n",
      "[24] loss: 0.337\n",
      "[25] loss: 0.315\n",
      "[26] loss: 0.311\n",
      "[27] loss: 0.305\n",
      "[28] loss: 0.293\n",
      "[29] loss: 0.288\n",
      "[30] loss: 0.284\n",
      "[31] loss: 0.278\n",
      "[32] loss: 0.273\n",
      "[33] loss: 0.268\n",
      "[34] loss: 0.261\n",
      "[35] loss: 0.256\n",
      "[36] loss: 0.249\n",
      "[37] loss: 0.243\n",
      "[38] loss: 0.237\n",
      "[39] loss: 0.231\n",
      "[40] loss: 0.225\n",
      "[41] loss: 0.220\n",
      "[42] loss: 0.216\n",
      "[43] loss: 0.210\n",
      "[44] loss: 0.205\n",
      "[45] loss: 0.201\n",
      "[46] loss: 0.197\n",
      "[47] loss: 0.198\n",
      "[48] loss: 0.197\n",
      "[49] loss: 0.186\n",
      "[50] loss: 0.183\n",
      "[51] loss: 0.180\n",
      "[52] loss: 0.177\n",
      "[53] loss: 0.173\n",
      "[54] loss: 0.172\n",
      "[55] loss: 0.168\n",
      "[56] loss: 0.166\n",
      "[57] loss: 0.164\n",
      "[58] loss: 0.161\n",
      "[59] loss: 0.159\n",
      "[60] loss: 0.157\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 60\n",
    "# training\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        outputs, hidden = model(inputs)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('[%d] loss: %.3f' % (epoch + 1, running_loss / 100))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model!\n",
    "\n",
    "We first check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test set: 92 %\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs, _ = model(inputs)\n",
    "        predicted = torch.round(outputs)\n",
    "        # print(predicted.sum(), labels.sum())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, accuracy isn't the only meaningful metric. As a result, we compute the confusion matrix below.\n",
    "\n",
    "What do the confusion matrix entries mean?\n",
    "\n",
    "(0,0): True Negative\n",
    "\n",
    "(0,1): False Positive\n",
    "\n",
    "(1,0): False Negative\n",
    "\n",
    "(1,1): True Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[427.,  25.],\n",
      "        [ 20., 159.]])\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "confusion = torch.zeros(2, 2)\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs, _ = model(inputs)\n",
    "        predicted = torch.round(outputs)\n",
    "        for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "            confusion[t.long(), p.long()] += 1\n",
    "\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.864130437374115\n",
      "Recall: 0.8882681727409363\n",
      "F1 score: 0.8760330677032471\n"
     ]
    }
   ],
   "source": [
    "precision = confusion[1, 1] / (confusion[1, 1] + confusion[0, 1])\n",
    "recall = confusion[1, 1] / (confusion[1, 1] + confusion[1, 0])\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print('Precision:', precision.item())\n",
    "print('Recall:', recall.item())\n",
    "print('F1 score:', f1.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy, precision, recall, and F1 score values are indicative of our successful training. There is obviously room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of spam emails in the training dataset is 27.0% while the percentage of spam emails in the testing dataset is 28.4%.\n"
     ]
    }
   ],
   "source": [
    "print('The percentage of spam emails in the training dataset is %.1f%% while the percentage of spam emails in the testing dataset is %.1f%%.' %((train_labels.sum()/len(train_labels)).item()*100, (test_labels.sum()/len(test_labels)).item()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the percentage of spam emails is approximately 28% in both training and testing datasets, we want out model to be at least 72% accurate. \n",
    "\n",
    "During my first attempts, my dataset wasn't balanced (82% non-spam, 18% spam) and my model would only predict \"0\" as the output. In that case, the 82% accuracy result was very misleading. It wasn't an indication of successful training, despite the high accuracy score. On the other hand, precision and recall were both 0, since there were no true positives. As a result, I had to balance my dataset by ignoring some of the non-spam emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "print('Model saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rgan_pytorch",
   "language": "python",
   "name": "rgan_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
